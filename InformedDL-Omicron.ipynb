{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06c6bda-73a9-4202-a182-42d1a66f2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "port numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748491ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "arguments=sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a2b631-d016-4d69-8774-33a454599233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd258a7acb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec5acd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path10=\"./data/DEU_Omicron_check.csv\"   \n",
    "path11= \"./data/FRA_Omicron_check.csv\"                     \n",
    "path16=\"./data/ITA_Omicron_check.csv\"                      \n",
    "path12=\"./results/covid_r_d_\"\n",
    "path13=\"./results/Getanp\"\n",
    "path14=\"./results/output_DINN_validation/\"                                     \n",
    "if not os.path.exists(path14):\n",
    "    os.makedirs(path14)\n",
    "if not os.path.exists(path12):\n",
    "    os.makedirs(path12)\n",
    "if not os.path.exists(path13):\n",
    "    os.makedirs(path13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18624fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TW_SIDR(sidr_data, s, e, pop):\n",
    "    d1 = pd.to_datetime(sidr_data['date'])[s:e]\n",
    "    d2 = sidr_data['S'][s:e]\n",
    "    d3 = sidr_data['I'][s:e]\n",
    "    d4 = sidr_data['H'][s:e]\n",
    "    d5 = sidr_data['D'][s:e]\n",
    "    d6 = sidr_data['R'][s:e]\n",
    "    d7 = sidr_data['V'][s:e]\n",
    "    date = []\n",
    "    S = []\n",
    "    I = []\n",
    "    H = []\n",
    "    D = []\n",
    "    R = []\n",
    "    V = []\n",
    "    timesteps = []\n",
    "    for item in range(s, e):\n",
    "        date.append(d1[item])\n",
    "        S.append(d2[item])  \n",
    "        I.append(d3[item]) \n",
    "        H.append(d4[item]) \n",
    "        D.append(d5[item])\n",
    "        R.append(d6[item])\n",
    "        V.append(d7[item])\n",
    "        timesteps.append(item-s)\n",
    "    sidr_data1 = np.asarray([date, timesteps, S, I, H, D, R, V]).T\n",
    "    columns = ['date', 't', 'S', 'I', 'H', 'D', 'R', 'V']\n",
    "    sidr_data = pd.DataFrame(sidr_data1, columns=columns)\n",
    "    return sidr_data\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(file_path, pop, window_size=90):\n",
    "    sidr_data = pd.read_csv(file_path, sep=',', encoding='utf-8')\n",
    "    ndays = window_size\n",
    "    j = 0\n",
    "    g = [None] * (len(sidr_data) - ndays)\n",
    "    for i in range(0, len(sidr_data) - ndays):\n",
    "        g[j] = TW_SIDR(sidr_data, i, i + ndays, pop)\n",
    "        j += 1\n",
    "    for i in range(len(g)):\n",
    "        g[i] = g[i].reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "\n",
    "def csvmaker(g, file_prefix):\n",
    "    #covid_cases_list = [] # create a list to store all covid_cases dataframes\n",
    "    #for i in range(len(g)):\n",
    "    for i in range(n, m):\n",
    "        data1 = np.asarray([g[i].S, g[i].I, g[i].H, g[i].D, g[i].R, g[i].V]).T\n",
    "        columns = ['Susceptible', 'Infected', 'Hospitalized', 'Deaths', 'Recovered', 'Vaccinated']\n",
    "        data1 = pd.DataFrame(data1, columns=columns)\n",
    "        data1.to_csv(f\"{path12}/{file_prefix}_{i}.csv\")#, index=False)\n",
    "        covid_cases = pd.read_csv(f\"{path12}/{file_prefix}_{i}.csv\", delimiter=',')\n",
    "        covid_cases.columns = ['t', 'susceptible','infected','hospitalized', 'dead', 'recovered','vaccinated'] #rename columns\n",
    "        covid_cases['t'] = covid_cases['t'].astype(float)\n",
    "    return covid_cases\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8804643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datatotimesteps(covid_cases):\n",
    "\n",
    "    susceptible = []\n",
    "    infected = []\n",
    "    hospitalized = []\n",
    "    \n",
    "    dead = []\n",
    "    recovered = []\n",
    "    vaccinated = []\n",
    "    timesteps = []\n",
    "\n",
    "    length = len(covid_cases['t'])\n",
    "    d1 = covid_cases['susceptible'][:length]\n",
    "    d2 = covid_cases['infected'][:length]\n",
    "    d3 = covid_cases['hospitalized'][:length]\n",
    "    d4 = covid_cases['dead'][:length]\n",
    "    d5 = covid_cases['recovered'][:length]\n",
    "    d6 = covid_cases['vaccinated'][:length]\n",
    "    d7 = covid_cases['t'][:length]\n",
    "\n",
    "    for item in range(len(d7)):\n",
    "        susceptible.append(d1[item])\n",
    "        infected.append(d2[item])\n",
    "        hospitalized.append(d3[item])\n",
    "        dead.append(d4[item])\n",
    "        recovered.append(d5[item])\n",
    "        vaccinated.append(d6[item])\n",
    "        timesteps.append(d7[item])\n",
    "    return timesteps, susceptible, infected, hospitalized, dead, recovered, vaccinated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7035728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, I_data, H_data, D_data, R_data, V_data, path1, lambda_val):\n",
    "        super(DINN, self).__init__()\n",
    "        for country_name, country_data in [(\"France\", data_FRA), (\"Italy\", data_ITA), (\"Germany\", data_DEU)]:\n",
    "            if country_name == \"Germany\":\n",
    "                pop = pop_DEU\n",
    "            elif country_name == \"France\":\n",
    "                pop = pop_FRA\n",
    "            elif country_name == \"Italy\":\n",
    "                pop = pop_ITA\n",
    "        \n",
    "        self.N = pop  # population size\n",
    "        \n",
    "        # Initialize tensors\n",
    "        self.t = torch.tensor(t, requires_grad=True).float()\n",
    "        self.t_batch = torch.reshape(self.t, (len(self.t), 1))  # reshape for batch\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.H = torch.tensor(H_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "        self.V = torch.tensor(V_data)\n",
    "\n",
    "\n",
    "        # Save path, losses, and lambda\n",
    "        self.losses = []\n",
    "        self.save = 3  # file to save\n",
    "        self.lambda_mse = lambda_val\n",
    "\n",
    "        # Normalization values\n",
    "        self.S_max = max(self.S)\n",
    "        self.I_max = max(self.I)\n",
    "        self.H_max = max(self.H)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.V_max = max(self.V)\n",
    "        \n",
    "        self.S_min = min(self.S)\n",
    "        self.I_min = min(self.I)\n",
    "        self.H_min = min(self.H)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "        self.V_min = min(self.V)\n",
    "\n",
    "        # Unnormalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.H_hat = (self.H - self.H_min) / (self.H_max - self.H_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)\n",
    "        self.V_hat = (self.V - self.V_min) / (self.V_max - self.V_min)\n",
    "\n",
    "    \n",
    "\n",
    "        # Neural network for SIHRDV model\n",
    "        self.net_sidr = self.Net_sidr()\n",
    "        self.params = list(self.net_sidr.parameters())\n",
    "\n",
    "    \n",
    "\n",
    "    # Neural network architecture\n",
    "    class Net_sidr(nn.Module):  # input = [t]\n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            # Define hidden layers as a sequential block\n",
    "            self.hidden_layers = nn.Sequential(\n",
    "                nn.Linear(1, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20)\n",
    "            )\n",
    "\n",
    "            # Define output layers\n",
    "            self.out = nn.Linear(20, 6)  # Outputs for S, I, H, D, R, V\n",
    "            self.param_out = nn.Linear(20, 7)  # 7 parameters: beta, delta, gammaR, gammaH, nuR, nuD, eta\n",
    "\n",
    "            # Apply proper weight initialization\n",
    "            self._initialize_weights()\n",
    "\n",
    "        def _initialize_weights(self):\n",
    "            \"\"\"Initialize weights properly using He initialization for ReLU and Xavier for outputs.\"\"\"\n",
    "            for layer in self.modules():\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    init.kaiming_uniform_(layer.weight, nonlinearity='relu')  # He init for ReLU\n",
    "                    init.zeros_(layer.bias)  # Set biases to 0\n",
    "\n",
    "            # Xavier init for final output layers\n",
    "            init.xavier_uniform_(self.out.weight)\n",
    "            init.zeros_(self.out.bias)\n",
    "            init.xavier_uniform_(self.param_out.weight)\n",
    "            init.zeros_(self.param_out.bias)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            \"\"\"Forward pass through the network.\"\"\"\n",
    "            x = self.hidden_layers(t_batch)  # Pass through hidden layers\n",
    "            sidr = self.out(x)  # Predict S, I, H, D, R, V\n",
    "            params = self.param_out(x)  # Predict model parameters\n",
    "            return sidr, params\n",
    "\n",
    "\n",
    "        \n",
    "    def net_f(self, t_batch, compute_gradients=True):\n",
    "        t_batch.requires_grad_(compute_gradients) \n",
    "        sidr_hat, param_hat = self.net_sidr(t_batch)\n",
    "\n",
    "        # Extract predictions from the network\n",
    "        S_hat, I_hat, H_hat, D_hat, R_hat, V_hat = torch.unbind(sidr_hat, dim=1) \n",
    "        # Extract parameter predictions\n",
    "        beta_hat, delta_hat, gammaR_hat, gammaH_hat, nuR_hat, nuD_hat, eta_hat = torch.unbind(param_hat, dim=1)\n",
    "        \n",
    "        # Apply constraints to parameters\n",
    "        self.beta = torch.abs(torch.tanh(beta_hat))  \n",
    "        self.delta = torch.abs(torch.tanh(delta_hat))\n",
    "        self.gammaR = torch.abs(torch.tanh(gammaR_hat))\n",
    "        self.gammaH = torch.abs(torch.tanh(gammaH_hat))\n",
    "        self.nuR = torch.abs(torch.tanh(nuR_hat))\n",
    "        self.nuD = torch.abs(torch.tanh(nuD_hat))\n",
    "        self.eta = torch.abs(torch.tanh(eta_hat))\n",
    "        \n",
    "        if not compute_gradients:\n",
    "            # If gradients are not needed, return only the predictions\n",
    "            return None, None, None, None, None, S_hat, I_hat, H_hat, D_hat, R_hat, V_hat\n",
    "\n",
    "        # Compute gradients for S, I, H, D, R, V\n",
    "        S_hat_t = torch.autograd.grad(S_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        I_hat_t = torch.autograd.grad(I_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        H_hat_t = torch.autograd.grad(H_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        D_hat_t = torch.autograd.grad(D_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        R_hat_t = torch.autograd.grad(R_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        V_hat_t = torch.autograd.grad(V_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        # Unnormalize predictions\n",
    "        S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "        I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "        H = self.H_min + (self.H_max - self.H_min) * H_hat\n",
    "        D = self.D_min + (self.D_max - self.D_min) * D_hat\n",
    "        R = self.R_min + (self.R_max - self.R_min) * R_hat\n",
    "        V = self.V_min + (self.V_max - self.V_min) * V_hat\n",
    "\n",
    "        # Compute physical constraints f1_hat, f2_hat, etc.\n",
    "        f1_hat = S_hat_t - (-self.beta  * S * I/ self.N - self.delta * V + self.eta * R) / (self.S_max - self.S_min)\n",
    "        f2_hat = I_hat_t - (self.beta * S * I/ self.N  - self.gammaH * I - self.gammaR * I) / (self.I_max - self.I_min)\n",
    "        f3_hat = H_hat_t - (self.gammaH * I - self.nuD * H - self.nuR * H) / (self.H_max - self.H_min)\n",
    "        f4_hat = D_hat_t - (self.nuD * H) / (self.D_max - self.D_min)\n",
    "        f5_hat = R_hat_t - (self.gammaR * I + self.nuR * H + self.delta * V - self.eta * R) / (self.R_max - self.R_min)\n",
    "\n",
    "        return f1_hat, f2_hat, f3_hat, f4_hat, f5_hat, S_hat, I_hat, H_hat, D_hat, R_hat, V_hat\n",
    "\n",
    "    def load(self,path1):\n",
    "      # Load checkpoint\n",
    "      try:\n",
    "        checkpoint = torch.load(path1 + str(self.save)+'.pt') \n",
    "        #print('\\nloading pre-trained model...')############\n",
    "        self.load_state_dict(checkpoint['model'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        epoch = checkpoint.get('epoch', 0)\n",
    "        self.losses = checkpoint.get('losses', [])\n",
    "         \n",
    "\n",
    "      except RuntimeError :\n",
    "          print('changed the architecture, ignore')\n",
    "          pass\n",
    "      except FileNotFoundError:\n",
    "          pass\n",
    "    \n",
    "    def find_lambda(self, adaptive_lambda, phy_loss, mse_loss, model, beta=0.9):\n",
    "        \"\"\"\n",
    "        Dynamically adjusts the lambda (weight) for the MSE loss to balance with PHY loss.\n",
    "        Parameters:\n",
    "            adaptive_lambda: Current lambda value\n",
    "            phy_loss: Physics-based loss\n",
    "            mse_loss: Data loss\n",
    "            beta: Momentum term for smooth updates (default: 0.9)\n",
    "        Returns:\n",
    "            Updated adaptive_lambda\n",
    "        \"\"\"\n",
    "        # Compute gradients of PHY loss\n",
    "        phy_grad = torch.autograd.grad(phy_loss, model.parameters(), retain_graph=True, allow_unused=True)\n",
    "        # Compute gradients of MSE loss\n",
    "        mse_grad = torch.autograd.grad(mse_loss, model.parameters(), retain_graph=True, allow_unused=True)\n",
    "\n",
    "        # Filter out None gradients\n",
    "        grad_phy_norm = torch.stack([g.norm() for g in phy_grad if g is not None]).max()\n",
    "        grad_mse_norm = torch.stack([g.norm() for g in mse_grad if g is not None]).mean()\n",
    "\n",
    "        # Calculate the new lambda as the ratio of gradient norms\n",
    "        lambda_new = grad_phy_norm / (grad_mse_norm + 1e-8)  # Add epsilon to avoid division by zero\n",
    "\n",
    "        # Smoothly update lambda using momentum\n",
    "        adaptive_lambda = (1 - beta) * adaptive_lambda + beta * lambda_new\n",
    "\n",
    "        return adaptive_lambda\n",
    "\n",
    "    def train(self, n_epochs, path1, iteration, country_name, l2_lambda=1e-5,\n",
    "          early_stopping_patience=15000, early_stopping_min_delta=1e-7):\n",
    "        \n",
    "        length = len(self.S_hat)\n",
    "        train_end = int(length * 0.9)  # Use 90% for training\n",
    "        val_start = train_end  # Validation starts from the end of the training set\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        patience = early_stopping_patience\n",
    "        \n",
    "        \n",
    "        # Initialize rolling window and monitoring parameters\n",
    "        physics_loss_window = []\n",
    "        monitor_window_size = 5  # Number of epochs to monitor\n",
    "        lambda_mse_min = 0.1  # Minimum value for lambda_mse\n",
    "        lambda_mse_max = 10.0  # Maximum value for lambda_mse\n",
    "        decay_factor = 0.5  # Scaling factor for decay\n",
    "        reset_threshold = 2  # Monitor physics loss for this many epochs after adjustment\n",
    "        reset_counter = 0  # Counter to track epochs since last adjustment\n",
    "        previous_lambda_mse = None  # Track previous lambda_mse value\n",
    "\n",
    "        \n",
    "        \n",
    "        # Prepare to track loss history\n",
    "        loss_history = {\n",
    "            'epoch': [],\n",
    "            'iteration': [],\n",
    "            'MSE_loss': [],\n",
    "            'PHY_loss': [],\n",
    "            'L2_loss': [],\n",
    "            'TOT_loss': [],\n",
    "            'val_loss': [],\n",
    "            'lambda_mse': []\n",
    "        }\n",
    "\n",
    "\n",
    "        self.load(path1)  # Load the model if checkpoint exists\n",
    "\n",
    "        MSE_loss = np.zeros(n_epochs)\n",
    "        PHY_loss = np.zeros(n_epochs)\n",
    "        L2_loss = np.zeros(n_epochs)\n",
    "        TOT_loss = np.zeros(n_epochs)\n",
    "        val_loss_history = np.zeros(n_epochs)\n",
    "\n",
    "        print('\\nStarting training...\\n')\n",
    "\n",
    "        best_val_loss = float('inf')  # For tracking the best validation loss for saving the model\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            S_train_list = []\n",
    "            I_train_list = []\n",
    "            H_train_list = []\n",
    "            D_train_list = []\n",
    "            R_train_list = []\n",
    "            V_train_list = []\n",
    "\n",
    "            \n",
    "            # Training step\n",
    "            self.optimizer.zero_grad()  # Reset gradients for the training step\n",
    "            f1, f2, f3, f4, f5, S_train_pred, I_train_pred, H_train_pred, D_train_pred, R_train_pred, V_train_pred = self.net_f(self.t_batch[:train_end])\n",
    "\n",
    "            # Compute losses on the training set \n",
    "            mse_loss_train = (F.mse_loss(self.S_hat[:train_end].float(), S_train_pred.float()) +\n",
    "                              F.mse_loss(self.I_hat[:train_end].float(), I_train_pred.float()) +\n",
    "                              F.mse_loss(self.H_hat[:train_end].float(), H_train_pred.float()) +\n",
    "                              F.mse_loss(self.D_hat[:train_end].float(), D_train_pred.float()) +\n",
    "                              F.mse_loss(self.R_hat[:train_end].float(), R_train_pred.float()) +\n",
    "                              F.mse_loss(self.V_hat[:train_end].float(), V_train_pred.float()))\n",
    "\n",
    "\n",
    "            phy_loss_train = (torch.mean(torch.square(f1)) +\n",
    "                              torch.mean(torch.square(f2)) +\n",
    "                              torch.mean(torch.square(f3)) +\n",
    "                              torch.mean(torch.square(f4)) +\n",
    "                              torch.mean(torch.square(f5)))\n",
    "            \n",
    "            # Append the unnormalized predictions to the lists after each training step\n",
    "            S_train_list.append(self.S_min + (self.S_max - self.S_min) * S_train_pred) \n",
    "            I_train_list.append(self.I_min + (self.I_max - self.I_min) * I_train_pred)\n",
    "            H_train_list.append(self.H_min + (self.H_max - self.H_min) * H_train_pred)\n",
    "            D_train_list.append(self.D_min + (self.D_max - self.D_min) * D_train_pred)\n",
    "            R_train_list.append(self.R_min + (self.R_max - self.R_min) * R_train_pred)\n",
    "            V_train_list.append(self.V_min + (self.V_max - self.V_min) * V_train_pred)\n",
    "\n",
    "            \n",
    "            self.lambda_mse = self.find_lambda(self.lambda_mse, phy_loss_train, mse_loss_train, self.net_sidr, beta=0.9)\n",
    "\n",
    "            self.lambda_mse = max(lambda_mse_min, min(self.lambda_mse, lambda_mse_max))\n",
    "            # Compute the total loss\n",
    "            l2_loss = sum(torch.sum(param ** 2) for param in self.net_sidr.parameters()) * l2_lambda\n",
    " \n",
    "            total_loss_train = (self.lambda_mse * mse_loss_train + phy_loss_train + l2_loss)\n",
    "            \n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            total_loss_train.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.net_sidr.parameters(), max_norm=5)  # Gradient clipping\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Add the current physics loss to the monitoring window\n",
    "            physics_loss_window.append(phy_loss_train.item())\n",
    "            if len(physics_loss_window) > monitor_window_size:\n",
    "                physics_loss_window.pop(0)\n",
    "\n",
    "            # Dynamically adjust lambda_mse if physics loss stagnates\n",
    "            if len(physics_loss_window) >= monitor_window_size:\n",
    "                avg_phy_loss = sum(physics_loss_window) / monitor_window_size\n",
    "                if all(phy_loss > avg_phy_loss for phy_loss in physics_loss_window[-monitor_window_size:]):\n",
    "                    print(f\"Physics loss stagnating. Reducing lambda_mse from {self.lambda_mse:.6f}.\")\n",
    "                    self.lambda_mse *= decay_factor\n",
    "                    self.lambda_mse = max(lambda_mse_min, self.lambda_mse)\n",
    "\n",
    "            # Reset lambda_mse if adjustment worsens physics loss\n",
    "            if previous_lambda_mse is not None:\n",
    "                reset_counter += 1\n",
    "                if reset_counter >= reset_threshold:\n",
    "                    avg_after_adjustment = sum(physics_loss_window[-reset_threshold:]) / reset_threshold\n",
    "                    if avg_after_adjustment > avg_phy_loss:\n",
    "                        print(f\"Physics loss worsened. Resetting lambda_mse to {previous_lambda_mse:.6f}.\")\n",
    "                        self.lambda_mse = previous_lambda_mse\n",
    "                        previous_lambda_mse = None\n",
    "                        reset_counter = 0\n",
    "\n",
    "\n",
    "            # Track training losses\n",
    "            MSE_loss[epoch] = mse_loss_train.detach().cpu().numpy()\n",
    "            PHY_loss[epoch] = phy_loss_train.detach().cpu().numpy()\n",
    "            L2_loss[epoch] = l2_loss.detach().cpu().numpy()\n",
    "            TOT_loss[epoch] = total_loss_train.detach().cpu().numpy()\n",
    "            \n",
    "            S_val_list = []\n",
    "            I_val_list = []\n",
    "            H_val_list = []\n",
    "            D_val_list = []\n",
    "            R_val_list = []\n",
    "            V_val_list = []\n",
    "\n",
    "\n",
    "            # Validation step\n",
    "            with torch.no_grad():\n",
    "                _, _, _, _, _, S_val_pred, I_val_pred, H_val_pred, D_val_pred, R_val_pred, V_val_pred = self.net_f(self.t_batch[val_start:], compute_gradients=False)\n",
    "                \n",
    "                \n",
    "                S_val_list.append(self.S_min + (self.S_max - self.S_min) * S_val_pred) \n",
    "                I_val_list.append(self.I_min + (self.I_max - self.I_min) * I_val_pred)\n",
    "                H_val_list.append(self.H_min + (self.H_max - self.H_min) * H_val_pred)\n",
    "                D_val_list.append(self.D_min + (self.D_max - self.D_min) * D_val_pred)\n",
    "                R_val_list.append(self.R_min + (self.R_max - self.R_min) * R_val_pred)\n",
    "                V_val_list.append(self.V_min + (self.V_max - self.V_min) * V_val_pred)\n",
    "\n",
    "            \n",
    "                mse_loss_val = (F.mse_loss(self.S_hat[val_start:].float(), S_val_pred.float()) +\n",
    "                                F.mse_loss(self.I_hat[val_start:].float(), I_val_pred.float()) +\n",
    "                                F.mse_loss(self.H_hat[val_start:].float(), H_val_pred.float()) +\n",
    "                                F.mse_loss(self.D_hat[val_start:].float(), D_val_pred.float()) +\n",
    "                                F.mse_loss(self.R_hat[val_start:].float(), R_val_pred.float()) +\n",
    "                                F.mse_loss(self.V_hat[val_start:].float(), V_val_pred.float()))\n",
    "\n",
    "                val_loss_history[epoch] = mse_loss_val.detach().cpu().numpy()\n",
    "\n",
    "            # Early stopping logic\n",
    "            if mse_loss_val < best_val_loss - early_stopping_min_delta:\n",
    "                best_val_loss = mse_loss_val\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            # Record losses\n",
    "            loss_history['epoch'].append(epoch)\n",
    "            loss_history['iteration'].append(iteration)  # Track the iteration (i)\n",
    "            loss_history['MSE_loss'].append(mse_loss_train.detach().cpu().numpy())\n",
    "            loss_history['PHY_loss'].append(phy_loss_train.detach().cpu().numpy())\n",
    "            loss_history['L2_loss'].append(l2_loss.detach().cpu().numpy())\n",
    "            loss_history['TOT_loss'].append(total_loss_train.detach().cpu().numpy())\n",
    "            loss_history['val_loss'].append(mse_loss_val.detach().cpu().numpy())\n",
    "            loss_history['lambda_mse'].append(self.lambda_mse)\n",
    "            \n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"[Epoch {epoch}/{n_epochs}] [MSE Loss: {mse_loss_train.detach().cpu().numpy()}] \"f\"[Physics Loss: {phy_loss_train.detach().cpu().numpy()}] \" \"f\"[Validation Loss: {mse_loss_val.detach().cpu().numpy()}]\")\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # Early stopping\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Validation Loss: {best_val_loss:.6f}\")\n",
    "                break\n",
    "\n",
    "            # Save the model periodically\n",
    "            if epoch % 5000 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model': self.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler': self.scheduler.state_dict(),\n",
    "                }, path1 + str(self.save) + '.pt')\n",
    "                self.save = 2 if self.save % 2 != 0 else 3\n",
    "        \n",
    "\n",
    "        # Save losses for the current iteration\n",
    "        csv_path = os.path.join(path14, f\"losses_{country_name}_{iteration}.csv\")\n",
    "        loss_df = pd.DataFrame(loss_history)\n",
    "\n",
    "        # Write the DataFrame to a new file for this iteration\n",
    "        loss_df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "        return S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7f8eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path13\n",
    "pop_DEU = 83369840.0\n",
    "pop_FRA = 67813000.0\n",
    "pop_ITA = 59037472.0\n",
    "\n",
    "window_size= 90\n",
    "data_DEU = prepare_data(path10, pop_DEU, window_size=window_size)\n",
    "data_FRA = prepare_data(path11, pop_FRA, window_size=window_size)\n",
    "data_ITA = prepare_data(path16, pop_ITA, window_size=window_size)\n",
    "\n",
    "def net(path, n, m, epoch=50000):\n",
    "    results = {}\n",
    "    params_dict = {}\n",
    "\n",
    "    for country_name, country_data in [(\"France\", data_FRA), (\"Italy\", data_ITA), (\"Germany\", data_DEU)]:\n",
    "        country_results = {}\n",
    "        country_params = {}\n",
    "        dinn_country = [None] * len(country_data)\n",
    "        print(f\"\\n=== Starting training for country: {country_name}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(n, m):\n",
    "            covid_cases = csvmaker(country_data, country_name)\n",
    "            steps = datatotimesteps(covid_cases)\n",
    "            print(f\"\\n=== Iteration {i} ===\")\n",
    "            \n",
    "            country_data[i].t, country_data[i].S, country_data[i].I, country_data[i].H, country_data[i].D, country_data[i].R, country_data[i].V = steps\n",
    "\n",
    "            path2 = os.path.join(path, country_name + \"_\" + str(i))\n",
    "            dinn_country[i] = DINN(country_data[i].t, country_data[i].S, country_data[i].I, country_data[i].H, country_data[i].D, country_data[i].R, country_data[i].V, path2, 1)\n",
    "            learning_rate = 1e-5\n",
    "            optimizer = optim.Adam(dinn_country[i].params, lr=learning_rate, weight_decay=1e-4)  # Added weight_decay for regularization\n",
    "            dinn_country[i].optimizer = optimizer\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(dinn_country[i].optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
    "            dinn_country[i].scheduler = scheduler\n",
    "            \n",
    "            try:\n",
    "                S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list = dinn_country[i].train(epoch, path2, iteration=i, country_name=country_name)\n",
    "            except EOFError:\n",
    "                if dinn_country[i].save == 2:\n",
    "                    dinn_country[i].save = 3\n",
    "                    S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list = dinn_country[i].train(epoch, path2,iteration=i,country_name=country_name)\n",
    "                elif dinn_country[i].save == 3:\n",
    "                    dinn_country[i].save = 2\n",
    "                    S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list = dinn_country[i].train(epoch, path2, iteration=i, country_name=country_name)\n",
    "\n",
    "            \n",
    "            key = f\"{country_name}_{i}\"\n",
    "            \n",
    "            # Store both training and validation results\n",
    "            country_results[key] = {\n",
    "                \"train\": (S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list),\n",
    "                \"val\": (S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list)\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # Store model parameters\n",
    "            country_params[key] = {\n",
    "                \"beta\": dinn_country[i].beta.tolist() if isinstance(dinn_country[i].beta, torch.Tensor) else dinn_country[i].beta,\n",
    "                \"delta\": dinn_country[i].delta.tolist() if isinstance(dinn_country[i].delta, torch.Tensor) else dinn_country[i].delta,\n",
    "                \"gammaR\": dinn_country[i].gammaR.tolist() if isinstance(dinn_country[i].gammaR, torch.Tensor) else dinn_country[i].gammaR,\n",
    "                \"gammaH\": dinn_country[i].gammaH.tolist() if isinstance(dinn_country[i].gammaH, torch.Tensor) else dinn_country[i].gammaH,\n",
    "                \"nuR\": dinn_country[i].nuR.tolist() if isinstance(dinn_country[i].nuR, torch.Tensor) else dinn_country[i].nuR,\n",
    "                \"nuD\": dinn_country[i].nuD.tolist() if isinstance(dinn_country[i].nuD, torch.Tensor) else dinn_country[i].nuD,\n",
    "                \"eta\": dinn_country[i].eta.tolist() if isinstance(dinn_country[i].eta, torch.Tensor) else dinn_country[i].eta,\n",
    "            }\n",
    "            \n",
    "\n",
    "        results[country_name] = country_results\n",
    "        params_dict[country_name] = country_params\n",
    "\n",
    "    return results, params_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1afebac-972b-477e-ace7-10740bde4d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting training for country: France\n",
      "\n",
      "=== Iteration 0 ===\n",
      "\n",
      "Starting training...\n",
      "\n",
      "[Epoch 0/30001] [MSE Loss: 8.993017196655273] [Physics Loss: 6208.2978515625] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 6.675270080566406]\n",
      "[Epoch 500/30001] [MSE Loss: 8.85954475402832] [Physics Loss: 332.8330993652344] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 7.462035655975342]\n",
      "[Epoch 1000/30001] [MSE Loss: 8.053935050964355] [Physics Loss: 58.79320526123047] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 6.376247406005859]\n",
      "[Epoch 1500/30001] [MSE Loss: 6.73502254486084] [Physics Loss: 12.107416152954102] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 5.326125144958496]\n",
      "[Epoch 2000/30001] [MSE Loss: 6.011662483215332] [Physics Loss: 6.466269493103027] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.704862117767334]\n",
      "[Epoch 2500/30001] [MSE Loss: 5.466337203979492] [Physics Loss: 4.387406826019287] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.276512145996094]\n",
      "[Epoch 3000/30001] [MSE Loss: 4.885414123535156] [Physics Loss: 3.1744658946990967] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.644643545150757]\n",
      "[Epoch 3500/30001] [MSE Loss: 4.315299034118652] [Physics Loss: 2.4620003700256348] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.085719347000122]\n",
      "[Epoch 4000/30001] [MSE Loss: 3.647794723510742] [Physics Loss: 1.7884454727172852] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.60384202003479]\n",
      "[Epoch 4500/30001] [MSE Loss: 3.2135543823242188] [Physics Loss: 1.4943033456802368] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.356445074081421]\n",
      "[Epoch 5000/30001] [MSE Loss: 2.9073448181152344] [Physics Loss: 1.215530514717102] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.1953072547912598]\n",
      "[Epoch 5500/30001] [MSE Loss: 2.6152119636535645] [Physics Loss: 1.0492079257965088] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.011155128479004]\n",
      "[Epoch 6000/30001] [MSE Loss: 2.392932653427124] [Physics Loss: 0.9058257341384888] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.8937091827392578]\n",
      "[Epoch 6500/30001] [MSE Loss: 2.1787588596343994] [Physics Loss: 0.8336192965507507] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7795525789260864]\n",
      "[Epoch 7000/30001] [MSE Loss: 2.0025951862335205] [Physics Loss: 0.801764726638794] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.702915072441101]\n",
      "[Epoch 7500/30001] [MSE Loss: 1.8911412954330444] [Physics Loss: 0.8193754553794861] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6633493900299072]\n",
      "[Epoch 8000/30001] [MSE Loss: 1.6771037578582764] [Physics Loss: 0.6609389185905457] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.600107192993164]\n",
      "[Epoch 8500/30001] [MSE Loss: 1.5297954082489014] [Physics Loss: 0.5752569437026978] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5793980360031128]\n",
      "[Epoch 9000/30001] [MSE Loss: 1.4266963005065918] [Physics Loss: 0.5342543721199036] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5709043741226196]\n",
      "[Epoch 9500/30001] [MSE Loss: 1.31844162940979] [Physics Loss: 0.46668362617492676] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5650031566619873]\n",
      "[Epoch 10000/30001] [MSE Loss: 1.2211917638778687] [Physics Loss: 0.39841753244400024] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.582916259765625]\n",
      "[Epoch 10500/30001] [MSE Loss: 1.125032663345337] [Physics Loss: 0.3300033509731293] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5674740076065063]\n",
      "[Epoch 11000/30001] [MSE Loss: 1.0362119674682617] [Physics Loss: 0.301848441362381] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5328465700149536]\n",
      "[Epoch 11500/30001] [MSE Loss: 0.9662051200866699] [Physics Loss: 0.28201043605804443] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5204415321350098]\n",
      "[Epoch 12000/30001] [MSE Loss: 0.9033642411231995] [Physics Loss: 0.2780086100101471] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5173271894454956]\n",
      "[Epoch 12500/30001] [MSE Loss: 0.8480677008628845] [Physics Loss: 0.26893189549446106] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5201956033706665]\n",
      "[Epoch 13000/30001] [MSE Loss: 0.8018450140953064] [Physics Loss: 0.2609557509422302] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5281659364700317]\n",
      "[Epoch 13500/30001] [MSE Loss: 0.7529540061950684] [Physics Loss: 0.25463587045669556] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5428529977798462]\n",
      "[Epoch 14000/30001] [MSE Loss: 0.7096574902534485] [Physics Loss: 0.2508423626422882] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5576871633529663]\n",
      "[Epoch 14500/30001] [MSE Loss: 0.6668052077293396] [Physics Loss: 0.24787244200706482] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5767568349838257]\n",
      "[Epoch 15000/30001] [MSE Loss: 0.6301820874214172] [Physics Loss: 0.2399814873933792] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5940816402435303]\n",
      "[Epoch 15500/30001] [MSE Loss: 0.5951675176620483] [Physics Loss: 0.2267974317073822] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6108866930007935]\n",
      "[Epoch 16000/30001] [MSE Loss: 0.5548456907272339] [Physics Loss: 0.22892946004867554] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6353864669799805]\n",
      "[Epoch 16500/30001] [MSE Loss: 0.5194334983825684] [Physics Loss: 0.20154978334903717] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6621718406677246]\n",
      "[Epoch 17000/30001] [MSE Loss: 0.48520728945732117] [Physics Loss: 0.1934289038181305] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6774595975875854]\n",
      "[Epoch 17500/30001] [MSE Loss: 0.4623718559741974] [Physics Loss: 0.1946786493062973] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.689464807510376]\n",
      "[Epoch 18000/30001] [MSE Loss: 0.4379224181175232] [Physics Loss: 0.19167156517505646] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7030961513519287]\n",
      "[Epoch 18500/30001] [MSE Loss: 0.41875407099723816] [Physics Loss: 0.18187908828258514] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7112579345703125]\n",
      "[Epoch 19000/30001] [MSE Loss: 0.40418630838394165] [Physics Loss: 0.19196327030658722] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7157245874404907]\n",
      "[Epoch 19500/30001] [MSE Loss: 0.38853776454925537] [Physics Loss: 0.16547134518623352] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7211015224456787]\n",
      "[Epoch 20000/30001] [MSE Loss: 0.3756541609764099] [Physics Loss: 0.15647360682487488] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7204855680465698]\n",
      "[Epoch 20500/30001] [MSE Loss: 0.3658515512943268] [Physics Loss: 0.1514875739812851] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7174674272537231]\n",
      "[Epoch 21000/30001] [MSE Loss: 0.3557232618331909] [Physics Loss: 0.16092506051063538] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7140357494354248]\n",
      "[Epoch 21500/30001] [MSE Loss: 0.3468557596206665] [Physics Loss: 0.1357775628566742] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7092214822769165]\n",
      "[Epoch 22000/30001] [MSE Loss: 0.33859947323799133] [Physics Loss: 0.12698233127593994] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7039815187454224]\n",
      "[Epoch 22500/30001] [MSE Loss: 0.3317049443721771] [Physics Loss: 0.12646996974945068] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6978567838668823]\n",
      "[Epoch 23000/30001] [MSE Loss: 0.3263993561267853] [Physics Loss: 0.11676590889692307] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6920164823532104]\n",
      "[Epoch 23500/30001] [MSE Loss: 0.32113033533096313] [Physics Loss: 0.11567512154579163] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6864533424377441]\n",
      "[Epoch 24000/30001] [MSE Loss: 0.3163397014141083] [Physics Loss: 0.10900378227233887] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6820671558380127]\n",
      "[Epoch 24500/30001] [MSE Loss: 0.3117488920688629] [Physics Loss: 0.10193838179111481] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.676421880722046]\n",
      "[Epoch 25000/30001] [MSE Loss: 0.3075371980667114] [Physics Loss: 0.09529753774404526] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.673292636871338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25500/30001] [MSE Loss: 0.30408474802970886] [Physics Loss: 0.09241192787885666] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6710407733917236]\n",
      "[Epoch 26000/30001] [MSE Loss: 0.3014328181743622] [Physics Loss: 0.08681473135948181] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.669088363647461]\n",
      "[Epoch 26500/30001] [MSE Loss: 0.29861658811569214] [Physics Loss: 0.08229248970746994] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6673102378845215]\n",
      "[Epoch 27000/30001] [MSE Loss: 0.2957674562931061] [Physics Loss: 0.07907526940107346] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.6651560068130493]\n",
      "Early stopping at epoch 27137. Best Validation Loss: 1.515768\n",
      "\n",
      "=== Starting training for country: Italy\n",
      "\n",
      "=== Iteration 0 ===\n",
      "\n",
      "Starting training...\n",
      "\n",
      "[Epoch 0/30001] [MSE Loss: 9.058526039123535] [Physics Loss: 2200.99462890625] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 6.446089744567871]\n",
      "[Epoch 500/30001] [MSE Loss: 8.51359748840332] [Physics Loss: 67.46908569335938] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 6.690745830535889]\n",
      "[Epoch 1000/30001] [MSE Loss: 6.307689189910889] [Physics Loss: 13.981121063232422] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.166779041290283]\n",
      "[Epoch 1500/30001] [MSE Loss: 5.326591968536377] [Physics Loss: 7.61817741394043] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.2001795768737793]\n",
      "[Epoch 2000/30001] [MSE Loss: 4.501038551330566] [Physics Loss: 4.465680122375488] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.4925475120544434]\n",
      "[Epoch 2500/30001] [MSE Loss: 4.101532936096191] [Physics Loss: 3.427701950073242] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.2443771362304688]\n",
      "[Epoch 3000/30001] [MSE Loss: 3.736048698425293] [Physics Loss: 2.6544032096862793] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.0330257415771484]\n",
      "[Epoch 3500/30001] [MSE Loss: 3.257932662963867] [Physics Loss: 2.0639524459838867] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.7808334827423096]\n",
      "[Epoch 4000/30001] [MSE Loss: 2.851576089859009] [Physics Loss: 1.719179391860962] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5748881101608276]\n",
      "[Epoch 4500/30001] [MSE Loss: 2.676055908203125] [Physics Loss: 1.514416217803955] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.4961082935333252]\n",
      "[Epoch 5000/30001] [MSE Loss: 2.357410430908203] [Physics Loss: 1.3840219974517822] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.3259375095367432]\n",
      "[Epoch 5500/30001] [MSE Loss: 2.165534019470215] [Physics Loss: 1.3194739818572998] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.2215888500213623]\n",
      "[Epoch 6000/30001] [MSE Loss: 1.9594762325286865] [Physics Loss: 1.2043644189834595] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.115361213684082]\n",
      "[Epoch 6500/30001] [MSE Loss: 1.7816410064697266] [Physics Loss: 1.0870370864868164] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0504902601242065]\n",
      "[Epoch 7000/30001] [MSE Loss: 1.5941522121429443] [Physics Loss: 0.9602673649787903] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0103533267974854]\n",
      "[Epoch 7500/30001] [MSE Loss: 1.4760780334472656] [Physics Loss: 0.9084848761558533] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0077745914459229]\n",
      "[Epoch 8000/30001] [MSE Loss: 1.3709381818771362] [Physics Loss: 0.859499990940094] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0073351860046387]\n",
      "[Epoch 8500/30001] [MSE Loss: 1.263658046722412] [Physics Loss: 0.8134780526161194] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0104742050170898]\n",
      "[Epoch 9000/30001] [MSE Loss: 1.1815558671951294] [Physics Loss: 0.7757804989814758] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0333281755447388]\n",
      "[Epoch 9500/30001] [MSE Loss: 1.0979630947113037] [Physics Loss: 0.7323045134544373] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0567443370819092]\n",
      "[Epoch 10000/30001] [MSE Loss: 1.006856918334961] [Physics Loss: 0.6240821480751038] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0777487754821777]\n",
      "[Epoch 10500/30001] [MSE Loss: 0.9336904883384705] [Physics Loss: 0.5624878406524658] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.084753155708313]\n",
      "[Epoch 11000/30001] [MSE Loss: 0.8703122138977051] [Physics Loss: 0.531631350517273] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0857527256011963]\n",
      "[Epoch 11500/30001] [MSE Loss: 0.8123500347137451] [Physics Loss: 0.5060204267501831] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0911155939102173]\n",
      "[Epoch 12000/30001] [MSE Loss: 0.745807945728302] [Physics Loss: 0.4905032217502594] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0861371755599976]\n",
      "[Epoch 12500/30001] [MSE Loss: 0.6985122561454773] [Physics Loss: 0.4763253331184387] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0823951959609985]\n",
      "[Epoch 13000/30001] [MSE Loss: 0.654880702495575] [Physics Loss: 0.4500889778137207] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.0890089273452759]\n",
      "[Epoch 13500/30001] [MSE Loss: 0.6127430200576782] [Physics Loss: 0.4348384439945221] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.099829077720642]\n",
      "[Epoch 14000/30001] [MSE Loss: 0.5702311992645264] [Physics Loss: 0.44390153884887695] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.1224541664123535]\n",
      "[Epoch 14500/30001] [MSE Loss: 0.5173468589782715] [Physics Loss: 0.4008936583995819] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.1584322452545166]\n",
      "[Epoch 15000/30001] [MSE Loss: 0.4859653115272522] [Physics Loss: 0.3994191586971283] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.1724402904510498]\n",
      "[Epoch 15500/30001] [MSE Loss: 0.46601665019989014] [Physics Loss: 0.38460028171539307] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.1861940622329712]\n",
      "[Epoch 16000/30001] [MSE Loss: 0.44568896293640137] [Physics Loss: 0.3545442819595337] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.201266884803772]\n",
      "[Epoch 16500/30001] [MSE Loss: 0.42402780055999756] [Physics Loss: 0.3183867037296295] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.2191152572631836]\n",
      "[Epoch 17000/30001] [MSE Loss: 0.3974030017852783] [Physics Loss: 0.28696441650390625] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.2521648406982422]\n",
      "[Epoch 17500/30001] [MSE Loss: 0.36486926674842834] [Physics Loss: 0.268978476524353] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.310197114944458]\n",
      "[Epoch 18000/30001] [MSE Loss: 0.3477637767791748] [Physics Loss: 0.250296413898468] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.3486353158950806]\n",
      "[Epoch 18500/30001] [MSE Loss: 0.3281942307949066] [Physics Loss: 0.24796468019485474] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.4010391235351562]\n",
      "[Epoch 19000/30001] [MSE Loss: 0.31733012199401855] [Physics Loss: 0.2230168581008911] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.4319331645965576]\n",
      "[Epoch 19500/30001] [MSE Loss: 0.31047436594963074] [Physics Loss: 0.22910016775131226] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.448613166809082]\n",
      "[Epoch 20000/30001] [MSE Loss: 0.30637091398239136] [Physics Loss: 0.21931877732276917] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.4629656076431274]\n",
      "[Epoch 20500/30001] [MSE Loss: 0.302006334066391] [Physics Loss: 0.2315666675567627] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.4746501445770264]\n",
      "[Epoch 21000/30001] [MSE Loss: 0.295490026473999] [Physics Loss: 0.21116743981838226] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.4914242029190063]\n",
      "[Epoch 21500/30001] [MSE Loss: 0.2884308993816376] [Physics Loss: 0.21710628271102905] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5093326568603516]\n",
      "[Epoch 22000/30001] [MSE Loss: 0.19997212290763855] [Physics Loss: 0.2564432621002197] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 1.5204317569732666]\n",
      "Early stopping at epoch 22185. Best Validation Loss: 0.999714\n",
      "\n",
      "=== Starting training for country: Germany\n",
      "\n",
      "=== Iteration 0 ===\n",
      "\n",
      "Starting training...\n",
      "\n",
      "[Epoch 0/30001] [MSE Loss: 10.806669235229492] [Physics Loss: 1870.3826904296875] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 11.851142883300781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 500/30001] [MSE Loss: 9.524730682373047] [Physics Loss: 162.21588134765625] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 12.026789665222168]\n",
      "[Epoch 1000/30001] [MSE Loss: 9.177656173706055] [Physics Loss: 76.44879150390625] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 11.806442260742188]\n",
      "[Epoch 1500/30001] [MSE Loss: 8.744161605834961] [Physics Loss: 39.68331527709961] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 11.492713928222656]\n",
      "[Epoch 2000/30001] [MSE Loss: 8.057441711425781] [Physics Loss: 21.376428604125977] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 11.030855178833008]\n",
      "[Epoch 2500/30001] [MSE Loss: 7.490823745727539] [Physics Loss: 14.436445236206055] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 10.551630973815918]\n",
      "[Epoch 3000/30001] [MSE Loss: 6.836945533752441] [Physics Loss: 10.317337989807129] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 9.951436042785645]\n",
      "[Epoch 3500/30001] [MSE Loss: 6.17241096496582] [Physics Loss: 7.420025825500488] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 9.238609313964844]\n",
      "[Epoch 4000/30001] [MSE Loss: 5.528079032897949] [Physics Loss: 5.425258636474609] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 8.556809425354004]\n",
      "[Epoch 4500/30001] [MSE Loss: 4.8865766525268555] [Physics Loss: 4.15330171585083] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 7.840843200683594]\n",
      "[Epoch 5000/30001] [MSE Loss: 4.2555060386657715] [Physics Loss: 2.9518210887908936] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 7.26503849029541]\n",
      "[Epoch 5500/30001] [MSE Loss: 3.7032527923583984] [Physics Loss: 2.2857613563537598] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 6.720678329467773]\n",
      "[Epoch 6000/30001] [MSE Loss: 3.172802686691284] [Physics Loss: 1.9736863374710083] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 6.285824298858643]\n",
      "[Epoch 6500/30001] [MSE Loss: 2.788540840148926] [Physics Loss: 1.6846143007278442] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 5.9075493812561035]\n",
      "[Epoch 7000/30001] [MSE Loss: 2.4672467708587646] [Physics Loss: 1.432369351387024] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 5.5820393562316895]\n",
      "[Epoch 7500/30001] [MSE Loss: 2.2016615867614746] [Physics Loss: 1.2715392112731934] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 5.31747579574585]\n",
      "[Epoch 8000/30001] [MSE Loss: 1.9763388633728027] [Physics Loss: 1.126591444015503] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 5.019155502319336]\n",
      "[Epoch 8500/30001] [MSE Loss: 1.8575611114501953] [Physics Loss: 1.082105278968811] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.927122116088867]\n",
      "[Epoch 9000/30001] [MSE Loss: 1.7367935180664062] [Physics Loss: 1.0066744089126587] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.783956527709961]\n",
      "[Epoch 9500/30001] [MSE Loss: 1.5905570983886719] [Physics Loss: 1.1523423194885254] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.699952602386475]\n",
      "[Epoch 10000/30001] [MSE Loss: 1.4614152908325195] [Physics Loss: 0.8778172135353088] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.605968475341797]\n",
      "[Epoch 10500/30001] [MSE Loss: 1.3689295053482056] [Physics Loss: 0.8074482083320618] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.522768497467041]\n",
      "[Epoch 11000/30001] [MSE Loss: 1.2822345495224] [Physics Loss: 0.7925537824630737] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.4612507820129395]\n",
      "[Epoch 11500/30001] [MSE Loss: 1.1976679563522339] [Physics Loss: 0.7642906904220581] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.367858409881592]\n",
      "[Epoch 12000/30001] [MSE Loss: 1.1188855171203613] [Physics Loss: 0.7274575233459473] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.307575702667236]\n",
      "[Epoch 12500/30001] [MSE Loss: 1.028015375137329] [Physics Loss: 0.6590033769607544] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.254233360290527]\n",
      "[Epoch 13000/30001] [MSE Loss: 0.9175220131874084] [Physics Loss: 0.6326966285705566] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.260457515716553]\n",
      "[Epoch 13500/30001] [MSE Loss: 0.863742470741272] [Physics Loss: 0.6099870204925537] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.247345924377441]\n",
      "[Epoch 14000/30001] [MSE Loss: 0.8172807693481445] [Physics Loss: 0.6067662239074707] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.175495147705078]\n",
      "[Epoch 14500/30001] [MSE Loss: 0.7476664185523987] [Physics Loss: 0.5883514881134033] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.144750118255615]\n",
      "[Epoch 15000/30001] [MSE Loss: 0.6956192851066589] [Physics Loss: 0.562714695930481] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.09468936920166]\n",
      "[Epoch 15500/30001] [MSE Loss: 0.649293065071106] [Physics Loss: 0.5415858030319214] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 4.006325721740723]\n",
      "[Epoch 16000/30001] [MSE Loss: 0.6035930514335632] [Physics Loss: 0.5558850765228271] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.9432926177978516]\n",
      "[Epoch 16500/30001] [MSE Loss: 0.5561455488204956] [Physics Loss: 0.5343469977378845] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.807243585586548]\n",
      "[Epoch 17000/30001] [MSE Loss: 0.5117892026901245] [Physics Loss: 0.4930378198623657] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.625574827194214]\n",
      "[Epoch 17500/30001] [MSE Loss: 0.47384920716285706] [Physics Loss: 0.48331218957901] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.522247076034546]\n",
      "[Epoch 18000/30001] [MSE Loss: 0.4406663179397583] [Physics Loss: 0.4298796057701111] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.4400253295898438]\n",
      "[Epoch 18500/30001] [MSE Loss: 0.41116833686828613] [Physics Loss: 0.3922513425350189] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.369157314300537]\n",
      "[Epoch 19000/30001] [MSE Loss: 0.3857189118862152] [Physics Loss: 0.38112199306488037] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.2978899478912354]\n",
      "[Epoch 19500/30001] [MSE Loss: 0.3615581691265106] [Physics Loss: 0.3449447751045227] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.2154853343963623]\n",
      "[Epoch 20000/30001] [MSE Loss: 0.34077438712120056] [Physics Loss: 0.35304689407348633] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.153083086013794]\n",
      "[Epoch 20500/30001] [MSE Loss: 0.3265862762928009] [Physics Loss: 0.32363462448120117] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.0871353149414062]\n",
      "[Epoch 21000/30001] [MSE Loss: 0.3054842948913574] [Physics Loss: 0.27068477869033813] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 3.0414998531341553]\n",
      "[Epoch 21500/30001] [MSE Loss: 0.284589022397995] [Physics Loss: 0.2501868009567261] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.9738664627075195]\n",
      "[Epoch 22000/30001] [MSE Loss: 0.26435360312461853] [Physics Loss: 0.30266764760017395] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.9406704902648926]\n",
      "[Epoch 22500/30001] [MSE Loss: 0.24237675964832306] [Physics Loss: 0.24859830737113953] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.910268545150757]\n",
      "[Epoch 23000/30001] [MSE Loss: 0.21565428376197815] [Physics Loss: 0.21334590017795563] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.9046497344970703]\n",
      "[Epoch 23500/30001] [MSE Loss: 0.19652222096920013] [Physics Loss: 0.18749964237213135] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.856968641281128]\n",
      "[Epoch 24000/30001] [MSE Loss: 0.1882021576166153] [Physics Loss: 0.2013239860534668] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.816117525100708]\n",
      "[Epoch 24500/30001] [MSE Loss: 0.1816534399986267] [Physics Loss: 0.17935329675674438] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.795825242996216]\n",
      "[Epoch 25000/30001] [MSE Loss: 0.17488206923007965] [Physics Loss: 0.16285060346126556] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.7330996990203857]\n",
      "[Epoch 25500/30001] [MSE Loss: 0.16523252427577972] [Physics Loss: 0.1681518256664276] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.6976985931396484]\n",
      "[Epoch 26000/30001] [MSE Loss: 0.1585090458393097] [Physics Loss: 0.1578383445739746] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.654517889022827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26500/30001] [MSE Loss: 0.15319417417049408] [Physics Loss: 0.14450065791606903] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.639282464981079]\n",
      "[Epoch 27000/30001] [MSE Loss: 0.14275144040584564] [Physics Loss: 0.14356999099254608] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.5979275703430176]\n",
      "[Epoch 27500/30001] [MSE Loss: 0.1338377147912979] [Physics Loss: 0.1399746835231781] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.56231951713562]\n",
      "[Epoch 28000/30001] [MSE Loss: 0.12511597573757172] [Physics Loss: 0.14019744098186493] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.5305535793304443]\n",
      "[Epoch 28500/30001] [MSE Loss: 0.11712167412042618] [Physics Loss: 0.1285639852285385] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.5001697540283203]\n",
      "[Epoch 29000/30001] [MSE Loss: 0.10971315205097198] [Physics Loss: 0.1247483640909195] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.472799777984619]\n",
      "[Epoch 29500/30001] [MSE Loss: 0.1020791307091713] [Physics Loss: 0.13715501129627228] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.440810441970825]\n",
      "[Epoch 30000/30001] [MSE Loss: 0.0962248370051384] [Physics Loss: 0.10657882690429688] [lambda_mse: 10.0] [L2 Loss: 1e-05] [Validation Loss: 2.4102420806884766]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n=0\n",
    "m=388#len(g)\n",
    "\n",
    "e=30001\n",
    "\n",
    "\n",
    "results = net (path,n,m,epoch=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc90b21f-2519-4911-92b9-3ea07d77ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f6a8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = {}\n",
    "\n",
    "for country, iterations in results[1].items():\n",
    "    if country not in data_frames:\n",
    "        # Initialize a DataFrame with columns for iterations and rows for parameters\n",
    "        data_frames[country] = pd.DataFrame(\n",
    "            columns=list(range(n, m)), \n",
    "            index=[\"Beta\", \"Delta\", \"GammaR\", \"GammaH\", \"NuR\", \"NuD\", \"Eta\"],\n",
    "            dtype=object\n",
    "        )\n",
    "\n",
    "    for iteration, params in iterations.items():\n",
    "        _, index = iteration.split('_')  # Extract the iteration index\n",
    "        index = int(index)\n",
    "\n",
    "        # Add the parameter lists to the corresponding column in the DataFrame\n",
    "        for param_name, param_values in params.items():\n",
    "            data_frames[country][int(index)] = params.values()\n",
    "            #data_frames[country].at[param_name.capitalize(), index] = param_values\n",
    "\n",
    "# Save each country's DataFrame to a CSV file\n",
    "for country, df in data_frames.items():\n",
    "    df.to_csv(f\"{path14}/Parameters_{n}_{m}_{country}.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24ab06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for predictions\n",
    "S_pred_list_train = {}\n",
    "I_pred_list_train = {}\n",
    "H_pred_list_train = {}\n",
    "D_pred_list_train = {}\n",
    "R_pred_list_train = {}\n",
    "V_pred_list_train = {}\n",
    "\n",
    "S_pred_list_val = {}\n",
    "I_pred_list_val = {}\n",
    "H_pred_list_val = {}\n",
    "D_pred_list_val = {}\n",
    "R_pred_list_val = {}\n",
    "V_pred_list_val = {}\n",
    "\n",
    "S_lists_train = {}\n",
    "I_lists_train = {}\n",
    "H_lists_train = {}\n",
    "D_lists_train = {}\n",
    "R_lists_train = {}\n",
    "V_lists_train = {}\n",
    "\n",
    "S_lists_val = {}\n",
    "I_lists_val = {}\n",
    "H_lists_val = {}\n",
    "D_lists_val = {}\n",
    "R_lists_val = {}\n",
    "V_lists_val = {}\n",
    "\n",
    "countries = [\"France\", \"Italy\", \"Germany\"]\n",
    "\n",
    "# Iterate through each country\n",
    "for country in countries:\n",
    "    # Initialize the lists for each country's training and validation predictions\n",
    "    S_pred_list_train[country] = []\n",
    "    I_pred_list_train[country] = []\n",
    "    H_pred_list_train[country] = []\n",
    "    D_pred_list_train[country] = []\n",
    "    R_pred_list_train[country] = []\n",
    "    V_pred_list_train[country] = []\n",
    "\n",
    "    S_pred_list_val[country] = []\n",
    "    I_pred_list_val[country] = []\n",
    "    H_pred_list_val[country] = []\n",
    "    D_pred_list_val[country] = []\n",
    "    R_pred_list_val[country] = []\n",
    "    V_pred_list_val[country] = []\n",
    "\n",
    "    country_data = results[0][country]\n",
    "\n",
    "    for i in range(n, m):\n",
    "        key = f\"{country}_{i}\"\n",
    "\n",
    "        if key in country_data:\n",
    "            train_pred = country_data[key]['train']\n",
    "            val_pred = country_data[key]['val']\n",
    "\n",
    "            # Append training predictions\n",
    "            S_pred_list_train[country].append(train_pred[0])\n",
    "            I_pred_list_train[country].append(train_pred[1])\n",
    "            H_pred_list_train[country].append(train_pred[2])\n",
    "            D_pred_list_train[country].append(train_pred[3])\n",
    "            R_pred_list_train[country].append(train_pred[4])\n",
    "            V_pred_list_train[country].append(train_pred[5])\n",
    "\n",
    "            # Append validation predictions\n",
    "            S_pred_list_val[country].append(val_pred[0])\n",
    "            I_pred_list_val[country].append(val_pred[1])\n",
    "            H_pred_list_val[country].append(val_pred[2])\n",
    "            D_pred_list_val[country].append(val_pred[3])\n",
    "            R_pred_list_val[country].append(val_pred[4])\n",
    "            V_pred_list_val[country].append(val_pred[5])\n",
    "\n",
    "    # Convert the tensors to lists for both training and validation sets\n",
    "    S_list_train = []\n",
    "    for tensor_sublist in S_pred_list_train[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        S_list_train.append(converted_sublist)\n",
    "    S_lists_train[country] = S_list_train\n",
    "\n",
    "    S_list_val = []\n",
    "    for tensor_sublist in S_pred_list_val[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        S_list_val.append(converted_sublist)\n",
    "    S_lists_val[country] = S_list_val\n",
    "\n",
    "    # Repeat the above conversion for all other categories (I, H, D, R, V)\n",
    "    I_list_train = []\n",
    "    for tensor_sublist in I_pred_list_train[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        I_list_train.append(converted_sublist)\n",
    "    I_lists_train[country] = I_list_train\n",
    "\n",
    "    I_list_val = []\n",
    "    for tensor_sublist in I_pred_list_val[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        I_list_val.append(converted_sublist)\n",
    "    I_lists_val[country] = I_list_val\n",
    "\n",
    "    # For H, D, R, V for both train and validation\n",
    "    # Training sets\n",
    "    H_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in H_pred_list_train[country]]\n",
    "    D_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in D_pred_list_train[country]]\n",
    "    R_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in R_pred_list_train[country]]\n",
    "    V_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in V_pred_list_train[country]]\n",
    "\n",
    "    # Validation sets\n",
    "    H_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in H_pred_list_val[country]]\n",
    "    D_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in D_pred_list_val[country]]\n",
    "    R_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in R_pred_list_val[country]]\n",
    "    V_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in V_pred_list_val[country]]\n",
    "\n",
    "# Save the predictions for both training and validation into CSV files\n",
    "country_dfs_train = {}\n",
    "country_dfs_val = {}\n",
    "\n",
    "for country in countries:\n",
    "    # Training DataFrame\n",
    "    df_train = pd.DataFrame([S_lists_train[country], I_lists_train[country], H_lists_train[country], D_lists_train[country], R_lists_train[country], V_lists_train[country]],\n",
    "                            index=[\"S_pred_list_train\", \"I_pred_list_train\", \"H_pred_list_train\", \"D_pred_list_train\", \"R_pred_list_train\", \"V_pred_list_train\"],\n",
    "                            columns=list(range(n, m)))\n",
    "    country_dfs_train[country] = df_train\n",
    "\n",
    "    # Validation DataFrame\n",
    "    df_val = pd.DataFrame([S_lists_val[country], I_lists_val[country], H_lists_val[country], D_lists_val[country], R_lists_val[country], V_lists_val[country]],\n",
    "                          index=[\"S_pred_list_val\", \"I_pred_list_val\", \"H_pred_list_val\", \"D_pred_list_val\", \"R_pred_list_val\", \"V_pred_list_val\"],\n",
    "                          columns=list(range(n, m)))\n",
    "    country_dfs_val[country] = df_val\n",
    "\n",
    "# Save to CSV for both training and validation results\n",
    "for country, df_train in country_dfs_train.items():\n",
    "    df_train.to_csv(path14 + f\"Predictions_train_{n}_{m}_{country}.csv\")\n",
    "\n",
    "for country, df_val in country_dfs_val.items():\n",
    "    df_val.to_csv(path14 + f\"Predictions_val_{n}_{m}_{country}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e9ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

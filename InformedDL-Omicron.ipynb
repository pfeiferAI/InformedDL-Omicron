{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06c6bda-73a9-4202-a182-42d1a66f2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748491ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "arguments=sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a2b631-d016-4d69-8774-33a454599233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff648797c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5acd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path10=\"./data/DEU_Omicron_check.csv\"   \n",
    "path11= \"./data/FRA_Omicron_check.csv\"                     \n",
    "path16=\"./data/ITA_Omicron_check.csv\"                      \n",
    "path12=\"./results/Actual_Compartments_each_Sliding_Window\"\n",
    "path13=\"./results/Checkpoints\"\n",
    "path14=\"./results/output_DINN_validation/\"                                     \n",
    "if not os.path.exists(path14):\n",
    "    os.makedirs(path14)\n",
    "if not os.path.exists(path12):\n",
    "    os.makedirs(path12)\n",
    "if not os.path.exists(path13):\n",
    "    os.makedirs(path13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18624fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TW_SIDR(sidr_data, s, e, pop):\n",
    "    d1 = pd.to_datetime(sidr_data['date'])[s:e]\n",
    "    d2 = sidr_data['S'][s:e]\n",
    "    d3 = sidr_data['I'][s:e]\n",
    "    d4 = sidr_data['H'][s:e]\n",
    "    d5 = sidr_data['D'][s:e]\n",
    "    d6 = sidr_data['R'][s:e]\n",
    "    d7 = sidr_data['V'][s:e]\n",
    "    date = []\n",
    "    S = []\n",
    "    I = []\n",
    "    H = []\n",
    "    D = []\n",
    "    R = []\n",
    "    V = []\n",
    "    timesteps = []\n",
    "    for item in range(s, e):\n",
    "        date.append(d1[item])\n",
    "        S.append(d2[item])  \n",
    "        I.append(d3[item]) \n",
    "        H.append(d4[item]) \n",
    "        D.append(d5[item])\n",
    "        R.append(d6[item])\n",
    "        V.append(d7[item])\n",
    "        timesteps.append(item-s)\n",
    "    sidr_data1 = np.asarray([date, timesteps, S, I, H, D, R, V]).T\n",
    "    columns = ['date', 't', 'S', 'I', 'H', 'D', 'R', 'V']\n",
    "    sidr_data = pd.DataFrame(sidr_data1, columns=columns)\n",
    "    return sidr_data\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(file_path, pop, window_size=90):\n",
    "    sidr_data = pd.read_csv(file_path, sep=',', encoding='utf-8')\n",
    "    ndays = window_size\n",
    "    j = 0\n",
    "    g = [None] * (len(sidr_data) - ndays)\n",
    "    for i in range(0, len(sidr_data) - ndays):\n",
    "        g[j] = TW_SIDR(sidr_data, i, i + ndays, pop)\n",
    "        j += 1\n",
    "    for i in range(len(g)):\n",
    "        g[i] = g[i].reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "\n",
    "def csvmaker(g, file_prefix):\n",
    "    #covid_cases_list = [] # create a list to store all covid_cases dataframes\n",
    "    #for i in range(len(g)):\n",
    "    for i in range(n, m):\n",
    "        data1 = np.asarray([g[i].S, g[i].I, g[i].H, g[i].D, g[i].R, g[i].V]).T\n",
    "        columns = ['Susceptible', 'Infected', 'Hospitalized', 'Deaths', 'Recovered', 'Vaccinated']\n",
    "        data1 = pd.DataFrame(data1, columns=columns)\n",
    "        data1.to_csv(f\"{path12}/{file_prefix}_{i}.csv\")#, index=False)\n",
    "        covid_cases = pd.read_csv(f\"{path12}/{file_prefix}_{i}.csv\", delimiter=',')\n",
    "        covid_cases.columns = ['t', 'susceptible','infected','hospitalized', 'dead', 'recovered','vaccinated'] #rename columns\n",
    "        covid_cases['t'] = covid_cases['t'].astype(float)\n",
    "    return covid_cases\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8804643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datatotimesteps(covid_cases):\n",
    "\n",
    "    susceptible = []\n",
    "    infected = []\n",
    "    hospitalized = []\n",
    "    \n",
    "    dead = []\n",
    "    recovered = []\n",
    "    vaccinated = []\n",
    "    timesteps = []\n",
    "\n",
    "    length = len(covid_cases['t'])\n",
    "    d1 = covid_cases['susceptible'][:length]\n",
    "    d2 = covid_cases['infected'][:length]\n",
    "    d3 = covid_cases['hospitalized'][:length]\n",
    "    d4 = covid_cases['dead'][:length]\n",
    "    d5 = covid_cases['recovered'][:length]\n",
    "    d6 = covid_cases['vaccinated'][:length]\n",
    "    d7 = covid_cases['t'][:length]\n",
    "\n",
    "    for item in range(len(d7)):\n",
    "        susceptible.append(d1[item])\n",
    "        infected.append(d2[item])\n",
    "        hospitalized.append(d3[item])\n",
    "        dead.append(d4[item])\n",
    "        recovered.append(d5[item])\n",
    "        vaccinated.append(d6[item])\n",
    "        timesteps.append(d7[item])\n",
    "    return timesteps, susceptible, infected, hospitalized, dead, recovered, vaccinated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7035728",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2607045494.py, line 383)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"[Epoch {epoch}/{n_epochs}] [MSE Loss: {mse_loss_train.detach().cpu().numpy()}] \"f\"[Physics Loss: {phy_loss_train.detach().cpu().numpy()}] \" \"f\"[Validation Loss: {mse_loss_val.detach().cpu().numpy()}]\")\u001b[0m\n\u001b[0m                                                                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, I_data, H_data, D_data, R_data, V_data, path1, lambda_val):\n",
    "        super(DINN, self).__init__()\n",
    "        for country_name, country_data in [(\"France\", data_FRA), (\"Italy\", data_ITA), (\"Germany\", data_DEU)]:\n",
    "            if country_name == \"Germany\":\n",
    "                pop = pop_DEU\n",
    "            elif country_name == \"France\":\n",
    "                pop = pop_FRA\n",
    "            elif country_name == \"Italy\":\n",
    "                pop = pop_ITA\n",
    "        \n",
    "        self.N = pop  # population size\n",
    "        \n",
    "        # Initialize tensors\n",
    "        self.t = torch.tensor(t, requires_grad=True).float()\n",
    "        self.t_batch = torch.reshape(self.t, (len(self.t), 1))  # reshape for batch\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.H = torch.tensor(H_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "        self.V = torch.tensor(V_data)\n",
    "\n",
    "\n",
    "        # Save path, losses, and lambda\n",
    "        self.losses = []\n",
    "        self.save = 3  # file to save\n",
    "        self.lambda_mse = lambda_val\n",
    "\n",
    "        # Normalization values\n",
    "        self.S_max = max(self.S)\n",
    "        self.I_max = max(self.I)\n",
    "        self.H_max = max(self.H)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.V_max = max(self.V)\n",
    "        \n",
    "        self.S_min = min(self.S)\n",
    "        self.I_min = min(self.I)\n",
    "        self.H_min = min(self.H)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "        self.V_min = min(self.V)\n",
    "\n",
    "        # Normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.H_hat = (self.H - self.H_min) / (self.H_max - self.H_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)\n",
    "        self.V_hat = (self.V - self.V_min) / (self.V_max - self.V_min)\n",
    "\n",
    "    \n",
    "\n",
    "        # Neural network for SIHRDV model\n",
    "        self.net_sidr = self.Net_sidr()\n",
    "        self.params = list(self.net_sidr.parameters())\n",
    "\n",
    "    \n",
    "\n",
    "    # Neural network architecture\n",
    "    class Net_sidr(nn.Module):  # input = [t]\n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            # Define hidden layers as a sequential block\n",
    "            self.hidden_layers = nn.Sequential(\n",
    "                nn.Linear(1, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20),\n",
    "                nn.Linear(20, 20), nn.ReLU(), nn.BatchNorm1d(20)\n",
    "            )\n",
    "\n",
    "            # Define output layers\n",
    "            self.out = nn.Linear(20, 6)  # Outputs for S, I, H, D, R, V\n",
    "            self.param_out = nn.Linear(20, 7)  # 7 parameters: beta, delta, gammaR, gammaH, nuR, nuD, eta\n",
    "\n",
    "            # Apply proper weight initialization\n",
    "            self._initialize_weights()\n",
    "\n",
    "        def _initialize_weights(self):\n",
    "            \"\"\"Initialize weights properly using He initialization for ReLU and Xavier for outputs.\"\"\"\n",
    "            for layer in self.modules():\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    init.kaiming_uniform_(layer.weight, nonlinearity='relu')  # He init for ReLU\n",
    "                    init.zeros_(layer.bias)  # Set biases to 0\n",
    "\n",
    "            # Xavier init for final output layers\n",
    "            init.xavier_uniform_(self.out.weight)\n",
    "            init.zeros_(self.out.bias)\n",
    "            init.xavier_uniform_(self.param_out.weight)\n",
    "            init.zeros_(self.param_out.bias)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            \"\"\"Forward pass through the network.\"\"\"\n",
    "            x = self.hidden_layers(t_batch)  # Pass through hidden layers\n",
    "            sidr = self.out(x)  # Predict S, I, H, D, R, V\n",
    "            params = self.param_out(x)  # Predict model parameters\n",
    "            return sidr, params\n",
    "\n",
    "\n",
    "        \n",
    "    def net_f(self, t_batch, compute_gradients=True):\n",
    "        t_batch.requires_grad_(compute_gradients) \n",
    "        sidr_hat, param_hat = self.net_sidr(t_batch)\n",
    "\n",
    "        # Extract predictions from the network\n",
    "        S_hat, I_hat, H_hat, D_hat, R_hat, V_hat = torch.unbind(sidr_hat, dim=1) \n",
    "        # Extract parameter predictions\n",
    "        beta_hat, delta_hat, gammaR_hat, gammaH_hat, nuR_hat, nuD_hat, eta_hat = torch.unbind(param_hat, dim=1)\n",
    "        \n",
    "        # Apply constraints to parameters\n",
    "        self.beta = torch.abs(torch.tanh(beta_hat))  \n",
    "        self.delta = torch.abs(torch.tanh(delta_hat))\n",
    "        self.gammaR = torch.abs(torch.tanh(gammaR_hat))\n",
    "        self.gammaH = torch.abs(torch.tanh(gammaH_hat))\n",
    "        self.nuR = torch.abs(torch.tanh(nuR_hat))\n",
    "        self.nuD = torch.abs(torch.tanh(nuD_hat))\n",
    "        self.eta = torch.abs(torch.tanh(eta_hat))\n",
    "        \n",
    "        if not compute_gradients:\n",
    "            # If gradients are not needed, return only the predictions\n",
    "            return None, None, None, None, None, S_hat, I_hat, H_hat, D_hat, R_hat, V_hat\n",
    "\n",
    "        # Compute gradients for S, I, H, D, R, V\n",
    "        S_hat_t = torch.autograd.grad(S_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        I_hat_t = torch.autograd.grad(I_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        H_hat_t = torch.autograd.grad(H_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        D_hat_t = torch.autograd.grad(D_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        R_hat_t = torch.autograd.grad(R_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "        V_hat_t = torch.autograd.grad(V_hat.sum(), t_batch, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        # Unnormalize predictions\n",
    "        S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "        I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "        H = self.H_min + (self.H_max - self.H_min) * H_hat\n",
    "        D = self.D_min + (self.D_max - self.D_min) * D_hat\n",
    "        R = self.R_min + (self.R_max - self.R_min) * R_hat\n",
    "        V = self.V_min + (self.V_max - self.V_min) * V_hat\n",
    "\n",
    "        # Compute physical constraints f1_hat, f2_hat, etc.\n",
    "        f1_hat = S_hat_t - (-self.beta * S_hat * I_hat / self.N - self.delta * V_hat + self.eta * R_hat)\n",
    "        f2_hat = I_hat_t - (self.beta * S_hat * I_hat / self.N - self.gammaH * I_hat - self.gammaR * I_hat)\n",
    "        f3_hat = H_hat_t - (self.gammaH * I_hat - self.nuD * H_hat - self.nuR * H_hat)\n",
    "        f4_hat = D_hat_t - (self.nuD * H_hat)\n",
    "        f5_hat = R_hat_t - (self.gammaR * I_hat + self.nuR * H_hat + self.delta * V_hat - self.eta * R_hat)\n",
    "        \n",
    "        return f1_hat, f2_hat, f3_hat, f4_hat, f5_hat, S_hat, I_hat, H_hat, D_hat, R_hat, V_hat\n",
    "\n",
    "    def load(self,path1):\n",
    "      # Load checkpoint\n",
    "      try:\n",
    "        checkpoint = torch.load(path1 + str(self.save)+'.pt') \n",
    "        #print('\\nloading pre-trained model...')############\n",
    "        self.load_state_dict(checkpoint['model'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        epoch = checkpoint.get('epoch', 0)\n",
    "        self.losses = checkpoint.get('losses', [])\n",
    "         \n",
    "\n",
    "      except RuntimeError :\n",
    "          print('changed the architecture, ignore')\n",
    "          pass\n",
    "      except FileNotFoundError:\n",
    "          pass\n",
    "    \n",
    "    def find_lambda(self, adaptive_lambda, phy_loss, mse_loss, model, beta=0.9):\n",
    "        \"\"\"\n",
    "        Dynamically adjusts the lambda (weight) for the MSE loss to balance with PHY loss.\n",
    "        Parameters:\n",
    "            adaptive_lambda: Current lambda value\n",
    "            phy_loss: Physics-based loss\n",
    "            mse_loss: Data loss\n",
    "            beta: Momentum term for smooth updates (default: 0.9)\n",
    "        Returns:\n",
    "            Updated adaptive_lambda\n",
    "        \"\"\"\n",
    "        # Compute gradients of PHY loss\n",
    "        phy_grad = torch.autograd.grad(phy_loss, model.parameters(), retain_graph=True, allow_unused=True)\n",
    "        # Compute gradients of MSE loss\n",
    "        mse_grad = torch.autograd.grad(mse_loss, model.parameters(), retain_graph=True, allow_unused=True)\n",
    "\n",
    "        # Filter out None gradients\n",
    "        phy_grad_norm = torch.stack([g.norm() for g in phy_grad if g is not None]).max()\n",
    "        mse_grad_norm = torch.stack([g.norm() for g in mse_grad if g is not None]).mean()\n",
    "        \n",
    "        \n",
    "        # Log-space scaling to reduce magnitude difference\n",
    "        lambda_ratio  = torch.exp(torch.log(grad_phy_norm) - torch.log(grad_mse_norm))\n",
    "\n",
    "        # Momentum smoothing (EMA)\n",
    "        adaptive_lambda = beta * adaptive_lambda + (1 - beta) * lambda_ratio\n",
    "\n",
    "        return adaptive_lambda\n",
    "\n",
    "    def train(self, n_epochs, path1, iteration, country_name, l2_lambda=1e-5,\n",
    "          early_stopping_patience=15000, early_stopping_min_delta=1e-7):\n",
    "        \n",
    "        length = len(self.S_hat)\n",
    "        train_end = int(length * 0.9)  # Use 90% for training\n",
    "        val_start = train_end  # Validation starts from the end of the training set\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        patience = early_stopping_patience\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Prepare to track loss history\n",
    "        loss_history = {\n",
    "            'epoch': [],\n",
    "            'iteration': [],\n",
    "            'MSE_loss': [],\n",
    "            'PHY_loss': [],\n",
    "            'L2_loss': [],\n",
    "            'TOT_loss': [],\n",
    "            'val_loss': [],\n",
    "            'lambda_mse': []\n",
    "        }\n",
    "\n",
    "\n",
    "        self.load(path1)  # Load the model if checkpoint exists\n",
    "\n",
    "        MSE_loss = np.zeros(n_epochs)\n",
    "        PHY_loss = np.zeros(n_epochs)\n",
    "        L2_loss = np.zeros(n_epochs)\n",
    "        TOT_loss = np.zeros(n_epochs)\n",
    "        val_loss_history = np.zeros(n_epochs)\n",
    "\n",
    "        print('\\nStarting training...\\n')\n",
    "\n",
    "        best_val_loss = float('inf')  # For tracking the best validation loss for saving the model\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            S_train_list = []\n",
    "            I_train_list = []\n",
    "            H_train_list = []\n",
    "            D_train_list = []\n",
    "            R_train_list = []\n",
    "            V_train_list = []\n",
    "\n",
    "            \n",
    "            # Training step\n",
    "            self.optimizer.zero_grad()  # Reset gradients for the training step\n",
    "            f1, f2, f3, f4, f5, S_train_pred, I_train_pred, H_train_pred, D_train_pred, R_train_pred, V_train_pred = self.net_f(self.t_batch[:train_end])\n",
    "\n",
    "            # Compute losses on the training set \n",
    "            mse_loss_train = (F.mse_loss(self.S_hat[:train_end].float(), S_train_pred.float()) +\n",
    "                              F.mse_loss(self.I_hat[:train_end].float(), I_train_pred.float()) +\n",
    "                              F.mse_loss(self.H_hat[:train_end].float(), H_train_pred.float()) +\n",
    "                              F.mse_loss(self.D_hat[:train_end].float(), D_train_pred.float()) +\n",
    "                              F.mse_loss(self.R_hat[:train_end].float(), R_train_pred.float()) +\n",
    "                              F.mse_loss(self.V_hat[:train_end].float(), V_train_pred.float()))\n",
    "\n",
    "\n",
    "            phy_loss_train = (torch.mean(torch.square(f1)) +\n",
    "                              torch.mean(torch.square(f2)) +\n",
    "                              torch.mean(torch.square(f3)) +\n",
    "                              torch.mean(torch.square(f4)) +\n",
    "                              torch.mean(torch.square(f5)))\n",
    "            \n",
    "            # Append the unnormalized predictions to the lists after each training step\n",
    "            S_train_list.append(self.S_min + (self.S_max - self.S_min) * S_train_pred) \n",
    "            I_train_list.append(self.I_min + (self.I_max - self.I_min) * I_train_pred)\n",
    "            H_train_list.append(self.H_min + (self.H_max - self.H_min) * H_train_pred)\n",
    "            D_train_list.append(self.D_min + (self.D_max - self.D_min) * D_train_pred)\n",
    "            R_train_list.append(self.R_min + (self.R_max - self.R_min) * R_train_pred)\n",
    "            V_train_list.append(self.V_min + (self.V_max - self.V_min) * V_train_pred)\n",
    "\n",
    "            \n",
    "            self.lambda_mse = self.find_lambda(self.lambda_mse, phy_loss_train, mse_loss_train, self.net_sidr, beta=0.9)\n",
    "\n",
    "            \n",
    "            # Compute the total loss\n",
    "            l2_loss = sum(torch.sum(param ** 2) for param in self.net_sidr.parameters()) * l2_lambda\n",
    " \n",
    "            total_loss_train = (self.lambda_mse * mse_loss_train + phy_loss_train + l2_loss)\n",
    "            \n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            total_loss_train.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.net_sidr.parameters(), max_norm=5)  # Gradient clipping\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "   \n",
    "            # Track training losses\n",
    "            MSE_loss[epoch] = mse_loss_train.detach().cpu().numpy()\n",
    "            PHY_loss[epoch] = phy_loss_train.detach().cpu().numpy()\n",
    "            L2_loss[epoch] = l2_loss.detach().cpu().numpy()\n",
    "            TOT_loss[epoch] = total_loss_train.detach().cpu().numpy()\n",
    "            \n",
    "            S_val_list = []\n",
    "            I_val_list = []\n",
    "            H_val_list = []\n",
    "            D_val_list = []\n",
    "            R_val_list = []\n",
    "            V_val_list = []\n",
    "\n",
    "\n",
    "            # Validation step\n",
    "            with torch.no_grad():\n",
    "                _, _, _, _, _, S_val_pred, I_val_pred, H_val_pred, D_val_pred, R_val_pred, V_val_pred = self.net_f(self.t_batch[val_start:], compute_gradients=False)\n",
    "                \n",
    "                \n",
    "                S_val_list.append(self.S_min + (self.S_max - self.S_min) * S_val_pred) \n",
    "                I_val_list.append(self.I_min + (self.I_max - self.I_min) * I_val_pred)\n",
    "                H_val_list.append(self.H_min + (self.H_max - self.H_min) * H_val_pred)\n",
    "                D_val_list.append(self.D_min + (self.D_max - self.D_min) * D_val_pred)\n",
    "                R_val_list.append(self.R_min + (self.R_max - self.R_min) * R_val_pred)\n",
    "                V_val_list.append(self.V_min + (self.V_max - self.V_min) * V_val_pred)\n",
    "\n",
    "            \n",
    "                mse_loss_val = (F.mse_loss(self.S_hat[val_start:].float(), S_val_pred.float()) +\n",
    "                                F.mse_loss(self.I_hat[val_start:].float(), I_val_pred.float()) +\n",
    "                                F.mse_loss(self.H_hat[val_start:].float(), H_val_pred.float()) +\n",
    "                                F.mse_loss(self.D_hat[val_start:].float(), D_val_pred.float()) +\n",
    "                                F.mse_loss(self.R_hat[val_start:].float(), R_val_pred.float()) +\n",
    "                                F.mse_loss(self.V_hat[val_start:].float(), V_val_pred.float()))\n",
    "\n",
    "                val_loss_history[epoch] = mse_loss_val.detach().cpu().numpy()\n",
    "\n",
    "            # Early stopping logic\n",
    "            if mse_loss_val < best_val_loss - early_stopping_min_delta:\n",
    "                best_val_loss = mse_loss_val\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            # Record losses\n",
    "            loss_history['epoch'].append(epoch)\n",
    "            loss_history['iteration'].append(iteration)  # Track the iteration (i)\n",
    "            loss_history['MSE_loss'].append(mse_loss_train.detach().cpu().numpy())\n",
    "            loss_history['PHY_loss'].append(phy_loss_train.detach().cpu().numpy())\n",
    "            loss_history['L2_loss'].append(l2_loss.detach().cpu().numpy())\n",
    "            loss_history['TOT_loss'].append(total_loss_train.detach().cpu().numpy())\n",
    "            loss_history['val_loss'].append(mse_loss_val.detach().cpu().numpy())\n",
    "            loss_history['lambda_mse'].append(self.lambda_mse)\n",
    "            \n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"[Epoch {epoch}/{n_epochs}] [MSE Loss: {mse_loss_train.detach().cpu().numpy()}] \"f\"[Physics Loss: {phy_loss_train.detach().cpu().numpy()}] \"f\"[lambda_mse: {self.lambda_mse}] \"f\"[L2 Loss: {l2_lambda}] \"f\"[Validation Loss: {mse_loss_val.detach().cpu().numpy()}]\")\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # Early stopping\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Validation Loss: {best_val_loss:.6f}\")\n",
    "                break\n",
    "\n",
    "            # Save the model periodically\n",
    "            if epoch % 5000 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model': self.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler': self.scheduler.state_dict(),\n",
    "                }, path1 + str(self.save) + '.pt')\n",
    "                self.save = 2 if self.save % 2 != 0 else 3\n",
    "        \n",
    "\n",
    "        # Save losses for the current iteration\n",
    "        csv_path = os.path.join(path14, f\"losses_{country_name}_{iteration}.csv\")\n",
    "        loss_df = pd.DataFrame(loss_history)\n",
    "\n",
    "        # Write the DataFrame to a new file for this iteration\n",
    "        loss_df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "        return S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f8eaf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/DEU_Omicron_check.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m pop_ITA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m59037472.0\u001b[39m\n\u001b[1;32m      6\u001b[0m window_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m\n\u001b[0;32m----> 7\u001b[0m data_DEU \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop_DEU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m data_FRA \u001b[38;5;241m=\u001b[39m prepare_data(path11, pop_FRA, window_size\u001b[38;5;241m=\u001b[39mwindow_size)\n\u001b[1;32m      9\u001b[0m data_ITA \u001b[38;5;241m=\u001b[39m prepare_data(path16, pop_ITA, window_size\u001b[38;5;241m=\u001b[39mwindow_size)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(file_path, pop, window_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(file_path, pop, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     sidr_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     ndays \u001b[38;5;241m=\u001b[39m window_size\n\u001b[1;32m     36\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DINN/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DINN/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DINN/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DINN/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DINN/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DINN/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DINN/lib/python3.9/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/DEU_Omicron_check.csv'"
     ]
    }
   ],
   "source": [
    "path = path13\n",
    "pop_DEU = 83369840.0\n",
    "pop_FRA = 67813000.0\n",
    "pop_ITA = 59037472.0\n",
    "\n",
    "window_size= 90\n",
    "data_DEU = prepare_data(path10, pop_DEU, window_size=window_size)\n",
    "data_FRA = prepare_data(path11, pop_FRA, window_size=window_size)\n",
    "data_ITA = prepare_data(path16, pop_ITA, window_size=window_size)\n",
    "\n",
    "def net(path, n, m, epoch=50000):\n",
    "    results = {}\n",
    "    params_dict = {}\n",
    "\n",
    "    for country_name, country_data in [(\"France\", data_FRA), (\"Italy\", data_ITA), (\"Germany\", data_DEU)]:\n",
    "        country_results = {}\n",
    "        country_params = {}\n",
    "        dinn_country = [None] * len(country_data)\n",
    "        print(f\"\\n=== Starting training for country: {country_name}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(n, m):\n",
    "            covid_cases = csvmaker(country_data, country_name)\n",
    "            steps = datatotimesteps(covid_cases)\n",
    "            print(f\"\\n=== Iteration {i} ===\")\n",
    "            \n",
    "            country_data[i].t, country_data[i].S, country_data[i].I, country_data[i].H, country_data[i].D, country_data[i].R, country_data[i].V = steps\n",
    "\n",
    "            path2 = os.path.join(path, country_name + \"_\" + str(i))\n",
    "            dinn_country[i] = DINN(country_data[i].t, country_data[i].S, country_data[i].I, country_data[i].H, country_data[i].D, country_data[i].R, country_data[i].V, path2, 1)\n",
    "            learning_rate = 1e-5\n",
    "            optimizer = optim.Adam(dinn_country[i].params, lr=learning_rate, weight_decay=1e-4)  # Added weight_decay for regularization\n",
    "            dinn_country[i].optimizer = optimizer\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(dinn_country[i].optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
    "            dinn_country[i].scheduler = scheduler\n",
    "            \n",
    "            try:\n",
    "                S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list = dinn_country[i].train(epoch, path2, iteration=i, country_name=country_name)\n",
    "            except EOFError:\n",
    "                if dinn_country[i].save == 2:\n",
    "                    dinn_country[i].save = 3\n",
    "                    S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list = dinn_country[i].train(epoch, path2,iteration=i,country_name=country_name)\n",
    "                elif dinn_country[i].save == 3:\n",
    "                    dinn_country[i].save = 2\n",
    "                    S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list, S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list = dinn_country[i].train(epoch, path2, iteration=i, country_name=country_name)\n",
    "\n",
    "            \n",
    "            key = f\"{country_name}_{i}\"\n",
    "            \n",
    "            # Store both training and validation results\n",
    "            country_results[key] = {\n",
    "                \"train\": (S_train_list, I_train_list, H_train_list, D_train_list, R_train_list, V_train_list),\n",
    "                \"val\": (S_val_list, I_val_list, H_val_list, D_val_list, R_val_list, V_val_list)\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # Store model parameters\n",
    "            country_params[key] = {\n",
    "                \"beta\": dinn_country[i].beta.tolist() if isinstance(dinn_country[i].beta, torch.Tensor) else dinn_country[i].beta,\n",
    "                \"delta\": dinn_country[i].delta.tolist() if isinstance(dinn_country[i].delta, torch.Tensor) else dinn_country[i].delta,\n",
    "                \"gammaR\": dinn_country[i].gammaR.tolist() if isinstance(dinn_country[i].gammaR, torch.Tensor) else dinn_country[i].gammaR,\n",
    "                \"gammaH\": dinn_country[i].gammaH.tolist() if isinstance(dinn_country[i].gammaH, torch.Tensor) else dinn_country[i].gammaH,\n",
    "                \"nuR\": dinn_country[i].nuR.tolist() if isinstance(dinn_country[i].nuR, torch.Tensor) else dinn_country[i].nuR,\n",
    "                \"nuD\": dinn_country[i].nuD.tolist() if isinstance(dinn_country[i].nuD, torch.Tensor) else dinn_country[i].nuD,\n",
    "                \"eta\": dinn_country[i].eta.tolist() if isinstance(dinn_country[i].eta, torch.Tensor) else dinn_country[i].eta,\n",
    "            }\n",
    "            \n",
    "\n",
    "        results[country_name] = country_results\n",
    "        params_dict[country_name] = country_params\n",
    "\n",
    "    return results, params_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afebac-972b-477e-ace7-10740bde4d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n=0\n",
    "m=388#len(g)\n",
    "\n",
    "e=30001\n",
    "\n",
    "\n",
    "results = net (path,n,m,epoch=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90b21f-2519-4911-92b9-3ea07d77ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = {}\n",
    "\n",
    "for country, iterations in results[1].items():\n",
    "    if country not in data_frames:\n",
    "        # Initialize a DataFrame with columns for iterations and rows for parameters\n",
    "        data_frames[country] = pd.DataFrame(\n",
    "            columns=list(range(n, m)), \n",
    "            index=[\"Beta\", \"Delta\", \"GammaR\", \"GammaH\", \"NuR\", \"NuD\", \"Eta\"],\n",
    "            dtype=object\n",
    "        )\n",
    "\n",
    "    for iteration, params in iterations.items():\n",
    "        _, index = iteration.split('_')  # Extract the iteration index\n",
    "        index = int(index)\n",
    "\n",
    "        # Add the parameter lists to the corresponding column in the DataFrame\n",
    "        for param_name, param_values in params.items():\n",
    "            data_frames[country][int(index)] = params.values()\n",
    "            #data_frames[country].at[param_name.capitalize(), index] = param_values\n",
    "\n",
    "# Save each country's DataFrame to a CSV file\n",
    "for country, df in data_frames.items():\n",
    "    df.to_csv(f\"{path14}/Parameters_{n}_{m}_{country}.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for predictions\n",
    "S_pred_list_train = {}\n",
    "I_pred_list_train = {}\n",
    "H_pred_list_train = {}\n",
    "D_pred_list_train = {}\n",
    "R_pred_list_train = {}\n",
    "V_pred_list_train = {}\n",
    "\n",
    "S_pred_list_val = {}\n",
    "I_pred_list_val = {}\n",
    "H_pred_list_val = {}\n",
    "D_pred_list_val = {}\n",
    "R_pred_list_val = {}\n",
    "V_pred_list_val = {}\n",
    "\n",
    "S_lists_train = {}\n",
    "I_lists_train = {}\n",
    "H_lists_train = {}\n",
    "D_lists_train = {}\n",
    "R_lists_train = {}\n",
    "V_lists_train = {}\n",
    "\n",
    "S_lists_val = {}\n",
    "I_lists_val = {}\n",
    "H_lists_val = {}\n",
    "D_lists_val = {}\n",
    "R_lists_val = {}\n",
    "V_lists_val = {}\n",
    "\n",
    "countries = [\"France\", \"Italy\", \"Germany\"]\n",
    "\n",
    "# Iterate through each country\n",
    "for country in countries:\n",
    "    # Initialize the lists for each country's training and validation predictions\n",
    "    S_pred_list_train[country] = []\n",
    "    I_pred_list_train[country] = []\n",
    "    H_pred_list_train[country] = []\n",
    "    D_pred_list_train[country] = []\n",
    "    R_pred_list_train[country] = []\n",
    "    V_pred_list_train[country] = []\n",
    "\n",
    "    S_pred_list_val[country] = []\n",
    "    I_pred_list_val[country] = []\n",
    "    H_pred_list_val[country] = []\n",
    "    D_pred_list_val[country] = []\n",
    "    R_pred_list_val[country] = []\n",
    "    V_pred_list_val[country] = []\n",
    "\n",
    "    country_data = results[0][country]\n",
    "\n",
    "    for i in range(n, m):\n",
    "        key = f\"{country}_{i}\"\n",
    "\n",
    "        if key in country_data:\n",
    "            train_pred = country_data[key]['train']\n",
    "            val_pred = country_data[key]['val']\n",
    "\n",
    "            # Append training predictions\n",
    "            S_pred_list_train[country].append(train_pred[0])\n",
    "            I_pred_list_train[country].append(train_pred[1])\n",
    "            H_pred_list_train[country].append(train_pred[2])\n",
    "            D_pred_list_train[country].append(train_pred[3])\n",
    "            R_pred_list_train[country].append(train_pred[4])\n",
    "            V_pred_list_train[country].append(train_pred[5])\n",
    "\n",
    "            # Append validation predictions\n",
    "            S_pred_list_val[country].append(val_pred[0])\n",
    "            I_pred_list_val[country].append(val_pred[1])\n",
    "            H_pred_list_val[country].append(val_pred[2])\n",
    "            D_pred_list_val[country].append(val_pred[3])\n",
    "            R_pred_list_val[country].append(val_pred[4])\n",
    "            V_pred_list_val[country].append(val_pred[5])\n",
    "\n",
    "    # Convert the tensors to lists for both training and validation sets\n",
    "    S_list_train = []\n",
    "    for tensor_sublist in S_pred_list_train[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        S_list_train.append(converted_sublist)\n",
    "    S_lists_train[country] = S_list_train\n",
    "\n",
    "    S_list_val = []\n",
    "    for tensor_sublist in S_pred_list_val[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        S_list_val.append(converted_sublist)\n",
    "    S_lists_val[country] = S_list_val\n",
    "\n",
    "    # Repeat the above conversion for all other categories (I, H, D, R, V)\n",
    "    I_list_train = []\n",
    "    for tensor_sublist in I_pred_list_train[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        I_list_train.append(converted_sublist)\n",
    "    I_lists_train[country] = I_list_train\n",
    "\n",
    "    I_list_val = []\n",
    "    for tensor_sublist in I_pred_list_val[country]:\n",
    "        converted_sublist = [tensor.tolist() for tensor in tensor_sublist]\n",
    "        I_list_val.append(converted_sublist)\n",
    "    I_lists_val[country] = I_list_val\n",
    "\n",
    "    # For H, D, R, V for both train and validation\n",
    "    # Training sets\n",
    "    H_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in H_pred_list_train[country]]\n",
    "    D_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in D_pred_list_train[country]]\n",
    "    R_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in R_pred_list_train[country]]\n",
    "    V_lists_train[country] = [[tensor.tolist() for tensor in sublist] for sublist in V_pred_list_train[country]]\n",
    "\n",
    "    # Validation sets\n",
    "    H_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in H_pred_list_val[country]]\n",
    "    D_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in D_pred_list_val[country]]\n",
    "    R_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in R_pred_list_val[country]]\n",
    "    V_lists_val[country] = [[tensor.tolist() for tensor in sublist] for sublist in V_pred_list_val[country]]\n",
    "\n",
    "# Save the predictions for both training and validation into CSV files\n",
    "country_dfs_train = {}\n",
    "country_dfs_val = {}\n",
    "\n",
    "for country in countries:\n",
    "    # Training DataFrame\n",
    "    df_train = pd.DataFrame([S_lists_train[country], I_lists_train[country], H_lists_train[country], D_lists_train[country], R_lists_train[country], V_lists_train[country]],\n",
    "                            index=[\"S_pred_list_train\", \"I_pred_list_train\", \"H_pred_list_train\", \"D_pred_list_train\", \"R_pred_list_train\", \"V_pred_list_train\"],\n",
    "                            columns=list(range(n, m)))\n",
    "    country_dfs_train[country] = df_train\n",
    "\n",
    "    # Validation DataFrame\n",
    "    df_val = pd.DataFrame([S_lists_val[country], I_lists_val[country], H_lists_val[country], D_lists_val[country], R_lists_val[country], V_lists_val[country]],\n",
    "                          index=[\"S_pred_list_val\", \"I_pred_list_val\", \"H_pred_list_val\", \"D_pred_list_val\", \"R_pred_list_val\", \"V_pred_list_val\"],\n",
    "                          columns=list(range(n, m)))\n",
    "    country_dfs_val[country] = df_val\n",
    "\n",
    "# Save to CSV for both training and validation results\n",
    "for country, df_train in country_dfs_train.items():\n",
    "    df_train.to_csv(path14 + f\"Predictions_train_{n}_{m}_{country}.csv\")\n",
    "\n",
    "for country, df_val in country_dfs_val.items():\n",
    "    df_val.to_csv(path14 + f\"Predictions_val_{n}_{m}_{country}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e9ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
